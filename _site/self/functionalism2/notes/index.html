<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Problems with the Turing Test &middot; Scott O'Connor
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="layout-reverse sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/Teaching/">Teaching</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/contact/">Contact</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/research/">Research</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    

  
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. Scott O'Connor. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Scott O'Connor</a>
            <small></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="page">
<nav>
   <h2><a href="/self">Course Home</a></h2>
</nav>
</div>
<div class="page">
  <h1 class="page-title">Problems with the Turing Test</h1>
  <h2 id="introduction">Introduction</h2>

<p>Turing claimed that being able to pass the Turing Test is necessary and sufficient for being able to think. I will paraphrase this as the claim that passing the Turing Test is necessary and sufficient for thought. If we can identify some entity that passes the test but cannot think, then passing the test is not sufficient for thought. Similarly, if we can identify something that fails to pass the test but can still think, then passing the test is not necessary for thought. Various attempts have been made to prove that passing the test is either not sufficient or not necessary for thought. For instance, some have asked whether there might be aliens who can think, but, nonetheless, cannot pass the test. The idea is that Turing may have excluded the possibility of there being very different types of thought in the universe.</p>

<p>The best argument of this type is the <strong>Chinese Room Thought Experiment</strong> by John Searle. This argument challengeds the claim that passing the test is sufficient for thought. Searle describes a situation where something performs the functional definition of understanding but fails to possess understanding. This seems to show that being able to perform some task is not sufficient for understanding. Since understanding is required for thought, it seems we could do some of the things associated with thought without being able to think.</p>

<p>Before we examine the details, a simple illustration of Searle’s strategy will help. Suppose that we have defined a heart functionally as anything whatsoever that pumps oxygenated blood to the tissues. Anything that performs that job should be a heart. Suppose, though, we find some machine that is being used to pump the blood of sick patient to their tissues. And further suppose that we are convinced that this machine is not a heart. We should conclude that being a heart is not merely defined by the job of pumping oxygenated blood to the tissues. Similarly, Searle focuses on the functional definition of understanding. He shows that at least some things can do the job of understanding without being a genuine instance of understanding.</p>

<h2 id="the-thought-experiment">The Thought Experiment</h2>

<p><img src="room.jpg" alt="image" /></p>

<p>The Chinese Room is designed to emulate a system that is analogous to a digital computer, a type of computing system. A Digital Computer is a machine intended to carry out any operation that could be done by a human computer. It has three parts:</p>

<dl>
  <dt>Store:</dt>
  <dd>a bank of information. It is like the paper or book that a human computer uses when doing their calculations.</dd>
  <dt>Executive unit:</dt>
  <dd>carries out the various operations.</dd>
  <dt>Control:</dt>
  <dd>like the rules or instructions for carrying out the calculation. Its job is to ensure that instructions are  obeyed correctly and in the right order.</dd>
</dl>

<p>Notice that we can dramatically increase the size and power of these three parts. So, even if we don’t yet have a powerful enough computer that passes the Turing Test, many think we might still  build one by increasing the power and size of these parts. However, Searle claims that no system with these parts has true thought, regardless of how powerful it is. In the thought experiment, Searle plays the role of the executive unit, the box of Chinese symbols plays the role of the store, the book of instructions is the control. Searle also adds two other features:</p>

<dl>
  <dt>Input:</dt>
  <dd>Chinese symbols that unbeknown to Searle are questions in Chinese.</dd>
  <dt>Output:</dt>
  <dd>Chinese symbols that unbeknown to Searle are answers in Chinese.</dd>
</dl>

<p><img src="directions.jpg" alt="image" /></p>

<p>Let us call the person in the room <em>Searle</em>. Searle will receive some symbol through the input box. Suppose those symbols represent a question. Searle does not know this, but he opens the very big and complicated instruction book. That book tells him that if he receives a certain symbol, or string of symbols, then he should take a different symbol, or string of symbols, and put it in the output box. Searle can do this. He even gets really quick and good at it.</p>

<p>Now, suppose that the person who inputs the symbols is a fluent speaker of Chinese. They are also the person who receives the outputted symbols. They ask a question like ‘who was Socrates?’ and then read as the output something like ‘Socrates was a Greek philosopher in the 4th century BC. He never wrote anything, but was the teacher of Plato, who reported his teacher’s conversations. Socrates was ultimately executed for corrupting the youth of Athens and inventing new Gods.’ Our Chinese speaker doesn’t know anything about what happens inside the room; they are unaware that Searle outputs various symbols by following an instruction book. Surely, the objection goes, our Chinese speaker believes that the person in the room understands Chinese. But the person in the room, Searle, does not understand Chinese. Searle summarizes the objection as follows:</p>

<blockquote>
  <p>Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese, but he does not understand a word of Chinese.</p>
</blockquote>

<p>It’s a clever argument. The thought-experiment models how computer systems work, how they output answers to the questions that we ask of them. But, if Searle doesn’t understand Chinese and can still outputs the correct answers, we have little reason to think that a computer system that outputs the correct answer to our questions has understanding. In order for us to present this as a formal argument, let us recall the difference between Strong and Weak AI:</p>

<dl>
  <dt>Weak Artificial Intelligence (WAI):</dt>
  <dd>Computers give us a powerful tool to study the mind. Thinking may be modeled by formal symbol systems, such as computer programs.</dd>
  <dt>Strong Artificial Intelligence (SAI):</dt>
  <dd>Thinking is constituted by the manipulation of formal symbols, such as occurs in a computer program.</dd>
</dl>

<p>WAI does not say that the computers really have a mind. It says only that such machines might resemble minds. It may also provide tools for investigating the mind by, for instance, giving us models that approximate the way the mind operates. But WAI does not say that computers have mental states; it does not state that computers realize mental states. On this view, Siri and Alexa mimic what minds can do, but our iPhones and Amazon devices do not have mental states just by having Siri and Alexa on them.  So, WAI cannot be used to support functionalism. On the other hand, SAI would give strong support for functionalism. If what it is to think just is to manipulate formal symbols, then thinking is a collection of functionally defined states. And, if such functions can be performed by both computers and human brains, then functionalism and Turing will be vindicated. Searle’s objection to SAI be presented as follows:</p>

<ol>
  <li>If SAI is true, then there is a program for Chinese such that if any computing system runs that program, that system thereby comes to understand Chinese.</li>
  <li>I could run a program for Chinese without thereby coming to
understand Chinese.</li>
  <li>Therefore, SAI is false.</li>
</ol>

<p>Premise 2 is demonstrated by the experiment. I could easily output the right responses without understanding Chinese. Premise 1 may seem surprising. Humans don’t seem to be computers. But, SAI claims that human minds really are computing devices. On this view, human brains and silicon chips are just two different ways of building the same thing: a computing device. Since we are a computing device, then we should understand Chinese if we fulfill the functional description of <em>understanding</em> Chinese, which is something like outputting the correct Chinese symbols when receiving questions in Chinese. Since we do not understand Chinese but do satisfy the functional definition for understanding Chinese, then satisfying this functional definition is not sufficient for such understanding. Generalizing, no computing device, human or machine, can think just by being able to satisfy the functional definition of thought.</p>

<h2 id="objections-to-searle">Objections to Searle</h2>

<p>Searle considers various worries with his  objection and responds to them. There are six in total:</p>

<ol>
  <li>The Systems Reply</li>
  <li>The Robot Reply</li>
  <li>The Brain Simulator Reply</li>
  <li>The Combination Reply</li>
  <li>The Other Minds Reply</li>
  <li>The Many Mansions Reply</li>
</ol>

<p>I will run through a few of the more interesting one.</p>

<h3 id="systems-reply">Systems Reply</h3>
<p><img src="system.jpg" alt="image" /></p>

<p><img src="computer.jpg" alt="image" /></p>

<p>The systems reply objects that even though the human inside of the
Chinese room doesn’t understand Chinese, nevertheless the entire system, including books and pieces of paper, does understand Chinese.</p>

<p>The reply, in effect, complains that Searle is committing what is called the fallacy of composition. This fallacy arises when one infers that something is true of the whole from the fact that it is true of some part of that whole. For example, this is a fallacy:</p>

<ol>
  <li>Searle’s hand weighs 2lb.</li>
  <li>Searle’s hand is a part of him.</li>
  <li>Thus, Searle weighs 2lb.</li>
</ol>

<p>An advocate of SAI claims that it is the computer as a whole that understands, not some piece of the computer. Likewise, they claim that the entire system that has Searle as a part might understand Chinese without it thereby following that Searle, the person in the room, understands Chinese. Similarly, the fact that one part of the system does not understand Chinese does not entail that the whole system fails to understands it.</p>

<p><img src="pouring.jpg" alt="image" /></p>

<p>But, Searle thinks that this objection, in addition to being incredible, overlooks the fact that the same thought-experiment can be run even if we assume that the human has internalized all of the books and other parts of the system.</p>

<h3 id="the-robot-reply">The Robot Reply</h3>

<p><img src="cat.jpg" alt="image" /></p>

<p>The Chinese Room represents a system that is meant to be similar to a computer system. Searle asks whether the system, or the person in the room, understands Chinese. Obviously, that person doesn’t and so Searle concludes that computer systems do not possess understanding. The Robot Reply accuses Searle of developing too simple a system. A real Chinese speaker interacts with the world. They move around it and perceive it. An advocate of SAI may claim that a computer that was connected to cameras and was able to move around the world could also possess understanding. If that’s the case, Searle’s thought-experiment doesn’t properly parallel the type of computer that could possess intelligence.</p>

<p><img src="Slide2.jpg" alt="image" /></p>

<p>But, Searle replies that we could easily change the thought experiment. Assume that the room is, in fact, part of the head of a gigantic robot. This robot moves around the world and has various parts that allow it gain information of that world. This information will update the data base–the store of symbols–and the program–the instructions for what do upon a certain input. Even when we make these changes, the person in the room does not understand Chinese.</p>

<h3 id="brain-simulator-reply">Brain Simulator Reply</h3>

<p>This reply concedes that Searle has shown that computer systems that work in the way that parallels the Chinese Room do not possess understanding. But, it claims that if we could make a computer that exactly parallels whatever goes on in a fluent Chinese speaker’s brain when he or she understands Chinese, then that computer would understand Chinese. What we need is for the electric circuitry of a computer to mimic the neural network of a Chinese speaker. According to this reply, if we could build such a computer, then it would understand Chinese.</p>

<p><img src="valves.jpg" alt="image" /></p>

<p>Searle’s response: we can change the thought experiment to show that such future computers would also not possess understanding. We will now change the experiment so that the man in the room turns on and shuts off water valves that correspond to the neural firings of a Chinese speaker. In such a case, the man is successfully outputting answers in Chinese and the mechanisms are parallel to the neural systems of a Chinese speaker. Nevertheless, the person in the room does not understand Chinese,</p>

</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
